<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-96CCSDGHTX"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());
        gtag('config', 'G-96CCSDGHTX');
    </script>

    <title>METRA</title>

    <meta name="author" content="Seohong Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <base target="_blank">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="../../stylesheet.css">
    <link rel="stylesheet" href="css/app.css?v=20231012">
    <link rel="icon" type="image/png" sizes="32x32" href="../../images/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../images/favicon/favicon-16x16.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
<div class="container" id="main">
    <div class="row">
        <h1 class="col-md-12 text-center">
            <b>METRA: Scalable Unsupervised RL with Metric-Aware Abstraction</b><br>
            <small>
                <b>ICLR 2024</b>
            </small>
        </h1>
    </div>

    <div class="row">
        <div class="col-md-12 text-center">
            <ul class="list-inline">
                <li>
                    <a href="https://seohong.me/">Seohong Park</a><br>UC Berkeley
                </li>
                <li>
                    <a href="https://people.eecs.berkeley.edu/~oleh/">Oleh Rybkin</a><br>UC Berkeley
                </li>
                <li>
                    <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><br>UC Berkeley
                </li>
            </ul>
        </div>
    </div>

    <div class="row">
        <div class="col-md-4 col-md-offset-4 text-center">
            <ul class="list-inline">
                <li>
                    <a href="https://arxiv.org/abs/2310.08887"><h4 class="top10"><b>Paper</b></h4></a>
                </li>
                <li>
                    <a href="https://github.com/seohongpark/METRA"><h4 class="top10"><b>Code</b></h4></a>
                </li>
            </ul>
        </div>
    </div>

    <div class="row">
        <div class="col-md-12">
            <h2>
                <span class="font-weight-normal">Abstract</span>
            </h2>
            <p class="text-justify">
                Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision.
                Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks.
                Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning.
                However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge:
                pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible,
                and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives.
                To make unsupervised RL scalable to complex, high-dimensional environments,
                we propose a novel unsupervised RL objective, which we call <b>Metric-Aware Abstraction</b> (<b>METRA</b>).
                Our main idea is, instead of directly covering the state space, to only cover a compact latent space \(\mathcal{Z}\) that is <i>metrically</i> connected to the state space \(\mathcal{S}\) by temporal distances.
                By learning to move in every direction in the latent space, METRA obtains a tractable set of diverse behaviors that approximately cover the state space, being scalable to high-dimensional environments.
                Through our experiments in five locomotion and manipulation environments, we demonstrate that METRA can discover a variety of useful behaviors even in complex, pixel-based environments,
                being the <b>first</b> unsupervised RL method that discovers diverse locomotion behaviors in pixel-based Quadruped and Humanoid.
            </p>
        </div>
    </div>

    <div class="row">
        <div class="col-md-12">
            <hr>
            <h2>Why might previous unsupervised RL methods fail to scale?</h2>
        </div>
        <div class="col-md-12">
            <br>
            <div class="col-md-12">
                <div class="col-md-12">
                    <p>
                        <img src="img/wdm.png" style="width: 100%;">
                    </p>
                </div>
            </div>
        </div>
        <div class="col-md-12">
            <ul>
                <li>Prior work in unsupervised RL can be categorized into two main groups: (1) <b>pure exploration methods</b> and (2) <b>unsupervised skill discovery methods.</b></li>
                <li>
                    <b>Pure exploration methods</b> aim to cover the <i>entire</i> state space or fully capture the environment dynamics.
                    However, in complex environments with a large state space, it is often infeasible to attain either of these aims.
                    In fact, we show that these methods fail to cover the state space even in the state-based 29-dimensional MuJoCo Ant environment.
                </li>
                <li>
                    <b>Unsupervised skill discovery methods</b> aim to discover diverse, distinguishable behaviors,
                    <i>e.g.</i>, by maximizing the mutual information (MI) between states and skills, \(I(S; Z)\).
                    While such MI-based skill learning methods do learn behaviors that are mutually different,
                    they do not necessarily encourage exploration and thus have limited state coverage.
                </li>
            </ul>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <hr>
            <h2>METRA: A <i>scalable</i> objective for unsupervised RL</h2>
        </div>
        <div class="col-md-12">
            <h3>Desiderata</h3>
            <ul>
                <li>Our main idea for scalable unsupervised RL is, instead of covering every possible state in the MDP,
                    which is infeasible in complex environments,
                    to learn <b>a small set of behaviors that collectively cover as much of the state space as possible</b>.
                </li>
            </ul>
            <h3>Objective</h3>
            <ul>
                <li>To achieve this, we propose the following novel objective for unsupervised RL:</li>
            </ul>
            <h3 class="text-center desktop-only">
                $$\begin{aligned}
                I_\mathcal{W}(S; Z) = \mathcal{W}(p(s, z), p(s)p(z)),
                \end{aligned}$$
            </h3>
            <span class="text-center mobile-only">
                $$\begin{aligned}
                I_\mathcal{W}(S; Z) = \mathcal{W}(p(s, z), p(s)p(z)),
                \end{aligned}$$
            </span>
            <ul>
                <li>where \(I_\mathcal{W}(S; Z)\) is the <b>Wasserstein dependency measure</b> (<b>WDM</b>) between states and skills, and \(\mathcal{W}\) is the 1-Wasserstein distance with the <b>temporal distance metric</b>.</li>
                <li>
                    Intuitively, \(I_\mathcal{W}(S; Z)\) can be viewed as a "Wasserstein variant" of the previous MI objective \(I(S; Z)\), where the KL divergence in MI is replaced with the Wasserstein distance.
                    However, despite the apparent similarity, there exists a significant difference between the two objectives: MI is completely agnostic to the underlying distance metric, while WDM is a <b>metric-aware</b> quantity.
                </li>
                <li>
                    As a result, the WDM objective not only discovers diverse skills that are different from one another, as in the MI objective,
                    but also actively maximizes temporal distances between different skill trajectories.
                    This makes them collectively cover the state space as much as possible.
                </li>
            </ul>
            <h3>Tractable optimization</h3>
            <ul>
                <li>Unfortunately, the WDM objective above is intractable. However, we show that it is equivalent to the following simple, tractable objective under some approximations:</li>
            </ul>
            <h3 class="text-center desktop-only">
                $$\begin{aligned}
                \text{Maximize} \ \ r = (\phi(s') - \phi(s))^\top z \quad
                \text{s.t.} \ \ \|\phi(s) - \phi(s')\|_2 \leq 1.
                \end{aligned}$$
            </h3>
            <span class="text-center mobile-only">
                $$\begin{aligned}
                \text{Maximize} \ \ &r = (\phi(s') - \phi(s))^\top z \quad \\
                \text{s.t.} \ \ &\|\phi(s) - \phi(s')\|_2 \leq 1.
                \end{aligned}$$
            </span>
            <ul>
                <li>We jointly train both a policy \(\pi(a|s, z)\) and a <i>representation</i> function \(\phi(s)\) with the <i>same</i> objective above using dual gradient descent.</li>
            </ul>
            <h3>Intuition</h3>
            <div class="col-md-12">
                <br>
                <div class="col-md-12">
                    <div class="col-md-10 col-md-offset-1">
                        <p>
                            <img src="img/illust.png" style="width: 100%;">
                        </p>
                    </div>
                </div>
            </div>
            <ul>
                <li>Our final objective above has an intuitive interpretation.
                    Intuitively, to maximize the reward, the policy should learn to move as far as possible along various directions (specified by \(z\)) in the latent space.</li>
                <li>Since distances in the latent space, \(\|\phi(s_1) - \phi(s_2)\|_2\), are always upper-bounded by the corresponding temporal distances in the MDP,
                    the latent space should assign its (limited) dimensions to the most "<b>temporally spread-out</b>" manifolds in the state space,
                    in the sense that shortest paths within the set of represented states should be as long as possible.</li>
                <li>
                    This conceptually resembles "<b>principal components</b>" of the state space, but with respect to shortest paths rather than Euclidean distances, and with non-linear \(\phi\) rather than linear \(\phi\).
                    In other words, \(\phi\) learns to <i>abstract</i> the state space in a lossy manner, while preserving the most temporally important manifolds.
                </li>
                <li>Based on this intuition, we call our method <b>Metric-Aware Abstraction</b> (<b>METRA</b>).</li>
            </ul>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <hr>
            <h2>How good is METRA?</h2>
        </div>
        <div class="col-md-12">
            <br>
            <div class="col-md-12">
                <div class="col-md-12">
                    <p>
                        <img src="img/qual.png" style="width: 100%;">
                    </p>
                </div>
            </div>
        </div>
        <div class="col-md-12">
            <br>
            <ul>
                <li>
                    To the best of our knowledge, METRA is the <b>first</b> unsupervised RL method that can discover diverse locomotion behaviors
                    in <b>pixel-based</b> Quadruped and Humanoid, without any prior knowledge, supervision, or data.
                </li>
            </ul>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <hr>
            <h2>Results (Pixel-based Quadruped)</h2>
        </div>
        <div class="col-md-12">
            <br>
            <div class="col-md-3 col-md-offset-3">
                <div class="col-md-12">
                    <p>
                        <img src="img/quadruped_ob.png" style="width: 100%;">
                    </p>
                </div>
                <span class="text-center text-caption top5 bottom5">
                    Observation (pixels)
                </span>
            </div>
            <div class="col-md-3">
                <div class="col-md-12">
                    <p>
                        <img src="img/quadruped_global.png" style="width: 100%">
                    </p>
                </div>
                <span class="text-center text-caption top5 bottom5">
                    Global view
                </span>
            </div>
        </div>
        <div class="col-md-12">
            <br>
            <ul>
                <li>We use gradient-colored floors to allow the agent to infer its location from pixel observations.</li>
            </ul>
        </div>
        <div class="col-md-12">
            <hr>
            <h3>Unsupervised skill discovery methods</h3>
        </div>
        <div class="col-md-12">
            <br>
            <div class="vertical-align">
                <div class="col-md-3">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_metra_16.png">
                    </p>
                </div>
                <div class="col-md-9">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_metra_16.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5">
                <b class="text-myred">METRA (ours)</b> (16 skills, 2 rollouts each)
            </span>
            <span class="text-center text-caption-small bottom5">
                To demonstrate the consistency of the learned skill policy, we show <b>two rollouts (videos) for each skill</b>.
            </span>
        </div>
        <div class="col-md-12">
            <br>
            <div class="vertical-align">
                <div class="col-md-3">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_metra_4d.png">
                    </p>
                </div>
                <div class="col-md-9">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_metra_4d.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5">
                <b class="text-myred">METRA (ours)</b> (4-D skills, 2 rollouts each for 9 randomly sampled skills)
            </span>
            <span class="text-center text-caption-small bottom5">
                To demonstrate the consistency of the learned skill policy, we show <b>two rollouts (videos) for each skill</b>.
            </span>
        </div>
        <div class="col-md-12">
            <br>
            <div class="vertical-align">
                <div class="col-md-3">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_diayn_4d.png">
                    </p>
                </div>
                <div class="col-md-9">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_diayn_4d.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5">
                <b>DIAYN</b> (4-D skills, 2 rollouts each for 9 randomly sampled skills)
            </span>
            <span class="text-center text-caption-small bottom5">
                To demonstrate the consistency of the learned skill policy, we show <b>two rollouts (videos) for each skill</b>.
            </span>
        </div>
        <div class="col-md-12">
            <br>
            <div class="vertical-align">
                <div class="col-md-3">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_lsd_4d.png">
                    </p>
                </div>
                <div class="col-md-9">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_lsd_4d.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5">
                <b>LSD</b> (4-D skills, 2 rollouts each for 9 randomly sampled skills)
            </span>
            <span class="text-center text-caption-small bottom5">
                To demonstrate the consistency of the learned skill policy, we show <b>two rollouts (videos) for each skill</b>.
            </span>
        </div>
        <div class="col-md-12">
            <br>
            <ul>
                <li>METRA discovers a variety of locomotion skills by maximizing <b>temporal distances</b> in diverse latent directions. In contrast, previous skill discovery methods mostly learn static behaviors.</li>
            </ul>
        </div>
        <div class="col-md-12">
            <hr>
            <h3>Unsupervised exploration methods</h3>
        </div>
        <div class="col-md-6">
            <br>
            <div class="vertical-align">
                <div class="col-md-6">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_icm.png">
                    </p>
                </div>
                <div class="col-md-6">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_icm.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5 bottom5">
                <b>ICM</b> (9 random rollouts)
            </span>
        </div>
        <div class="col-md-6">
            <br>
            <div class="vertical-align">
                <div class="col-md-6">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_rnd.png">
                    </p>
                </div>
                <div class="col-md-6">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_rnd.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5 bottom5">
                <b>RND</b> (9 random rollouts)
            </span>
        </div>
        <div class="col-md-6">
            <br>
            <div class="vertical-align">
                <div class="col-md-6">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_apt.png">
                    </p>
                </div>
                <div class="col-md-6">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_apt.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5 bottom5">
                <b>APT</b> (9 random rollouts)
            </span>
        </div>
        <div class="col-md-6">
            <br>
            <div class="vertical-align">
                <div class="col-md-6">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_aps.png">
                    </p>
                </div>
                <div class="col-md-6">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_aps.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5 bottom5">
                <b>APS</b> (9 random rollouts)
            </span>
        </div>
        <div class="col-md-6">
            <br>
            <div class="vertical-align">
                <div class="col-md-6">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_p2e.png">
                    </p>
                </div>
                <div class="col-md-6">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_p2e.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5 bottom5">
                <b>Plan2Explore</b> (9 random rollouts)
            </span>
        </div>
        <div class="col-md-6">
            <br>
            <div class="vertical-align">
                <div class="col-md-6">
                    <p>
                        <img class="img-responsive center-block" src="img/dqd_lbs.png">
                    </p>
                </div>
                <div class="col-md-6">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dqd_lbs.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5 bottom5">
                <b>LBS</b> (9 random rollouts)
            </span>
        </div>
        <div class="col-md-12">
            <br>
            <ul>
                <li>
                    Unsupervised exploration methods mostly exhibit chaotic, random behaviors and fail to fully explore the state space (in terms of \(x\)-\(y\) coordinates).
                    This is because it is practically infeasible to completely cover the infinitely many combinations of joint angles and positions.
                </li>
            </ul>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <hr>
            <h2>Results (Pixel-based Cheetah)</h2>
        </div>
        <div class="col-md-12">
            <br>
            <div class="vertical-align">
                <div class="col-md-3">
                    <p>
                        <img class="img-responsive center-block" src="img/dch_metra_8.png">
                    </p>
                </div>
                <div class="col-md-9">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dch_metra_8.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5">
                <b class="text-myred">METRA (ours)</b> (8 skills, 2 rollouts each)
            </span>
            <span class="text-center text-caption-small bottom5">
                To demonstrate the consistency of the learned skill policy, we show <b>two rollouts (videos) for each skill</b>.
            </span>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <hr>
            <h2>Results (Pixel-based Humanoid)</h2>
        </div>
        <div class="col-md-12">
            <br>
            <div class="vertical-align">
                <div class="col-md-3">
                    <p>
                        <img class="img-responsive center-block" src="img/dhu_metra_16.png">
                    </p>
                </div>
                <div class="col-md-9">
                    <p>
                        <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                            <source src="img/dhu_metra_16.mp4" type="video/mp4">
                        </video>
                    </p>
                </div>
            </div>
            <span class="text-center text-caption top5">
                <b class="text-myred">METRA (ours)</b> (16 skills, 2 rollouts each)
            </span>
            <span class="text-center text-caption-small bottom5">
                To demonstrate the consistency of the learned skill policy, we show <b>two rollouts (videos) for each skill</b>.
            </span>
        </div>
        <div class="col-md-12">
            <br>
            <ul>
                <li>METRA discovers diverse behaviors (e.g., running, backflipping, crawling, etc.) in pixel-based Cheetah and Humanoid as well. We refer to the paper for the complete qualitative results (8 seeds) on locomotion environments.</li>
            </ul>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <hr>
            <h2>Results (Pixel-based Kitchen)</h2>
        </div>
        <div class="col-md-12">
            <br>
            <div class="col-md-12">
                <p>
                    <video class="center-block" width="100%" autoplay loop muted controls playsinline>
                        <source src="img/kit_metra_24.mp4" type="video/mp4">
                    </video>
                </p>
            </div>
            <span class="text-center text-caption top5">
                <b class="text-myred">METRA (ours)</b> (24 skills, 2 rollouts each)
            </span>
            <span class="text-center text-caption-small bottom5">
                To demonstrate the consistency of the learned skill policy, we show <b>two rollouts (videos) for each skill</b>.
            </span>
        </div>
        <div class="col-md-12">
            <br>
            <ul>
                <li>METRA learns a variety of manipulation skills in pixel-based Kitchen. On average, METRA achieves 3-4 out of 6 predefined tasks (e.g., open the microwave, turn on the light, move the kettle, etc.).</li>
            </ul>
        </div>
    </div>

    <div class="row">
        <div class="col-md-12">
            <hr>
            <p class="text-justify">
                The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a> and <a href="https://jonbarron.info/">Jon Barron</a>.
            </p>
        </div>
    </div>

    <script>
        if (navigator.userAgent.match(/Android/i)
            || navigator.userAgent.match(/webOS/i)
            || navigator.userAgent.match(/iPhone/i)
            || navigator.userAgent.match(/iPad/i)
            || (navigator.userAgent.includes("Mac") && "ontouchend" in document)
            || navigator.userAgent.match(/iPod/i)
            || navigator.userAgent.match(/BlackBerry/i)
            || navigator.userAgent.match(/Windows Phone/i)) {
            const cells = document.getElementsByTagName('video');
            for (const cell of cells) {
                cell.removeAttribute("controls");
            }
        }
    </script>
</div>
</body>
</html>
