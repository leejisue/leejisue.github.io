<!DOCTYPE html>
<html>
<head>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">

  <meta content="width=device-width, initial-scale=1" name="viewport">
  <title>CARLA Semantic Segmentation Challenge</title>

  <!-- Google Tag Manager -->
  <script async="" src="http://www.google-analytics.com/analytics.js"></script>
  <script async="" src="https://www.googletagmanager.com/gtm.js?id=GTM-THP5XBK"></script>
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || [];
      w[l].push({
          'gtm.start':
              new Date().getTime(), event: 'gtm.js'
      });
      var f = d.getElementsByTagName(s)[0],
          j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : '';
      j.async = true;
      j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
      f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-THP5XBK');</script>
  <!-- End Google Tag Manager -->

  <link href="./css/fontawesome.all.min.css" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet">
  <link href="./css/bulma.min.css" rel="stylesheet">
  <link href="./css/index.css" rel="stylesheet">

  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/index.js"></script>

  <script>
      (function (i, s, o, g, r, a, m) {
          i['GoogleAnalyticsObject'] = r;
          i[r] = i[r] || function () {
              (i[r].q = i[r].q || []).push(arguments)
          }, i[r].l = 1 * new Date();
          a = s.createElement(o),
              m = s.getElementsByTagName(o)[0];
          a.async = 1;
          a.src = g;
          m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

      ga('create', 'UA-72422365-1', 'auto');
      ga('send', 'pageview');
  </script>

    <!--callout script-->
    <!-- Compressed CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/foundation-sites@6.7.4/dist/css/foundation.min.css"
          crossorigin="anonymous">

    <!-- Compressed JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/foundation-sites@6.7.4/dist/js/foundation.min.js"
            crossorigin="anonymous"></script>

</head>
<body>


<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://leejisue.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Project
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://leejisue.github.io/portfolio/carla-semantic-segmentation-challenge">
            CARLA semantic segmentation challenge
          </a>
          <a class="navbar-item" href="https://leejisue.github.io/portfolio/meta-rl-adas-challenge">
            CARLA meta-RL ADAS challenge
          </a>
          <a class="navbar-item" href="https://leejisue.github.io/portfolio/learn-to-race-adas-challenge">
            AIcrowd ADAS challenge
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>


<section class="hero publication-header">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-two-thirds">
          <div class="is-size-6 publication-venue">Portfolio</div>

          <h1 class="title is-1 publication-title">CARLA Semantic Segmentation Challenge</h1>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="content">

      <div class="columns is-centered">
        <div class="column is-two-thirds">
          <h2 class="is-2">Video</h2>
          <div class="publication-video">
            <iframe allow="autoplay; encrypted-media"
                    allowfullscreen frameborder="0"
                    src="https://www.youtube.com/embed/tlzcq1KYXd8?rel=0&amp;showinfo=0"></iframe>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-half">
          <h2>Introduction</h2>
            <hr/>
            <blockquote>In digital image processing and computer vision,
                <strong><span style="color: red;">Image segmentation</span></strong> is the process of partitioning a digital image into multiple image segments, also known as <strong>image regions</strong> or <strong>image objects</strong> (set of pixels).
                (<em>Wikipedia</em>)</blockquote>
            <p>
                Segmentation models are useful for a variety of tasks, including:
            </p>
            <ol>
                <li>
                    <strong>Autonomous Driving</strong> (<strong>AD</strong>)
                    <p>
                        Image segmentation in autonomous driving helps classify every pixel of a scene, allowing vehicles to understand their environment in detail and make safer decisions on the road.
                    </p>
                </li>
                <li>
                    <strong>Medical Imaging</strong>
                    <p>
                        Image segmentation in medical imaging allows for precise delineation of structures like tumors, blood vessels, or organs. This granularity helps in accurate diagnosis, treatment planning, and disease monitoring.
                    </p>
                </li>

                <li>
                    <strong>Agriculture</strong>
                    <p>
                       In agriculture, image segmentation can detect and differentiate between crops and weeds. This facilitates targeted herbicide application, ensuring healthier crop growth while minimizing chemical usage.
                    </p>
                </li>
            </ol>

        </div>
      </div>

      <div class="columns is-centered">
        <div class="column is-two-thirds">
          <div class="publication-body">
            <h2 class="is-2">Goal</h2>
              <hr/>
              <p>
                  Our goal is to achieve real-time semantic segmentation targets in the CARLA simulation environment using the diverse data collected from it. (e.g., heavy rain, fine dust, sunny, etc.)
              </p>
              <p>
                  <strong>The techniques that are utilized in this project</strong>:
              </p>

              <li>
                  Pretrained
                  <ol>
                      <li>
                          DeepLabv3 (backbone: resnet_50)
                      </li>
                      <li>
                          DeepLabv3 (backbone: resnet_50 with Conditional Random Field)
                      </li>
                      <li>
                          DeepLabv3 (backbone: mobilenet_v3_large)
                      </li>
                  </ol>
              </li>
              <li>
                  Scratch
                  <ol>
                      <li>
                          U-Net
                      </li>
                      <li>
                          Mask-RCNN
                      </li>
                  </ol>
              </li>

              <h2 class="is-2">Dataset Information</h2>
              <hr/>
              <p>
                  We performed segmentation using data collected from 10 towns provided by default in CARLA, under three environmental conditions: <strong><em>sunny</em></strong>, <strong><em>rainy</em></strong>, <strong><em>dusty</em></strong>.
              </p>
              <p>
                  The image data has a 1:2 ratio, and we were able to test the generalization performance of segmentation by adjusting to a maximum size of (216 x 512).
              </p>

              <h3 class="is-3">Class Definition</h3>
              <p>
                  We modified the <strong><em>28 classes</em></strong> obtained from the official CARLA documentation into <strong><em>12 classes</em></strong>.
              </p>
              <pre><code class="lang-bibtex">original_class = {0: "None", 1 : "Roads", 2 : "Sidewalks", 3 : "Buildings", 4 : "Other", 5 : "Other", 6 : "Poles", 7 : "TrafficLight", 8 : "TrafficSigns", 9 : "Vegetation", 10 : "Roads", 11 : "None", 12 : "Pedestrians", 13 : "Vehicles", 14 : "Vehicles", 15 : "Vehicles", 16 : "Vehicles", 17 : "Vehicles", 18 : "Vehicles", 19 : "Vehicles", 20 : "Other", 21 : "Other", 22 : "Other", 23 : "Other", 24 : "RoadLines", 25 : "Sidewalks", 26 : "Other", 27 : "Other", 28 : "Other"}
              remap_class = {0: "None", 1: "Roads", 2: "Sidewalks", 3: "Buildings", 4: "Other", 5: "Poles", 6: "TrafficLight", 7: "TrafficSigns", 8: "Vegetation", 9: "Pedestrians", 10: "Vehicles", 11: "RoadLines"}
              </code></pre>
              <p>
                  You can view the video of the original class and the remapped mask below.
              </p>
              <iframe src="https://wandb.ai/brunoleej/semantic_segmentation/reports/Naive-DeepLabv3_resnet50-Town04-Town06-Town10---Vmlldzo1MjIxNTk0?accessToken=t3jirjy8b4qme2z2j0vie609fl5qxgnkz5zbo07ulchf26jxlbd5u32g2l0dnrzu" width="100%" height="600px" frameborder="0"></iframe>

              <p>
                  There are several factors in images that influence the training of segmentation.
              </p>

              <ol>
                  <li>
                      <strong>Image Resolution</strong>:
                  </li>
                  <ol>
                      High-resolution images provide finer details but increase training time and require more memory.
                  </ol>
                  <ol>
                      Reducing resolution simplifies computation but may lead to loss of details.
                  </ol>

                  <li>
                      <strong>Image Quality</strong>:
                  </li>
                  <ol>
                      Poor quality images (e.g., with lots of noise or low contrast) can impact the accuracy of segmentation.
                  </ol>

                  <li>
                      <strong>Image Augmentation</strong>:
                  </li>
                  <ol>
                      Augmentation techniques (like rotation, scaling, flipping, brightness adjustments) help the model generalize better across varied scenarios.
                  </ol>
                  <ol>
                      Over-augmenting can risk overfitting the model.
                  </ol>

                  <li>
                      Class Imbalance:
                  </li>
                  <ol>
                      If certain classes of pixels vastly outnumber others in an image, it can lead to class imbalance issues. This might degrade segmentation accuracy for some classes.
                  </ol>

                  <li>
                      Annotation Quality:
                  </li>
                  <ol>
                      The quality of the ground truth segmentation masks plays a major role in the outcome of the training. Inaccurate masks can decrease training accuracy.
                  </ol>

                  <li>
                      Channel Information:
                  </li>
                  <ol>
                      Multi-channel images (e.g., RGB, infra-red, depth, etc.) can provide additional information, potentially improving segmentation accuracy.
                  </ol>

                  <li>
                      Variability and Diversity:
                  </li>
                  <ol>
                      A diverse set of images in the training set (varying lighting, angles, backgrounds, object sizes, etc.) ensures the model generalizes well in real-world scenarios.
                  </ol>

                  <li>
                      Contextual Information:
                  </li>
                  <ol>
                      Context in images can assist in predicting the position of specific objects or structures, especially crucial in larger images.
                  </ol>

                  <li>
                      Spatial Dependencies:
                  </li>
                  <ol>
                      Considering the spatial dependencies between pixels within an image can lead to more accurate segmentation outcomes.
                  </ol>

              </ol>

              <h2 class="is-2">Conclusion</h2>
              <hr/>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>This website template was borrowed from <a href="https://latentfusion.github.io/" style="font-size: inherit;">LatentFusion</a></p>
    </div>
  </div>
</footer>


</body>
</html>