<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>CARLA Semantic Segmentation Challenge</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style>
</head>
<body>
<article id="58615fe1-e99f-4d44-ade4-197a3ec118fb" class="page sans">
    <header><img class="page-cover-image" src="./imgs/ref_sensors_semantic.jpg" style="object-position:center 50%"/>
        <h1 class="page-title">CARLA Semantic Segmentation Challenge</h1>
        <p class="page-description"></p>
    </header>
    <div class="page-body">
    <h1 id="7d8f470a-830c-4c37-805f-a1ecc5eb5147" class="">Introduction</h1>
        <hr id="f748c7fc-fdc2-4158-b8c4-dc52ed2cf2d9"/>
        <figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="0f44d8af-118c-4317-9b53-ff3312243c81">
            <div style="font-size:1.5em">
                <span class="icon">ðŸ’¡</span>
            </div>
            <div style="width:100%">In digital image processing and computer vision, <mark class="highlight-red"><strong>Image segmentation</strong></mark> is the process of partitioning a digital image into multiple <strong>image segments</strong>, also known as <strong>image regions</strong> or <strong>image objects</strong> (set of pixels). (<em>Wikipedia</em>)</div></figure><p id="058894d5-7bee-4aa8-a3fa-15d734e07b39" class="">
</p><p id="96e545a7-21db-44a4-be07-14e4481e5874" class="">Segmentation models are useful for a variety of tasks, including:</p><ol type="1" id="4959d096-9575-4b25-a58c-f25c181d9542" class="numbered-list" start="1"><li><strong>Autonomous Driving</strong><p id="abf777e1-64ad-4f53-8e17-58a209d62617" class="">Image segmentation in autonomous driving helps classify every pixel of a scene, allowing vehicles to understand their environment in detail and make safer decisions on the road.</p></li></ol><ol type="1" id="cbf1fe54-0102-4405-a540-0c20d1c78bb2" class="numbered-list" start="2"><li><strong>Medical Imaging</strong><p id="407f2016-64a0-41cd-b20e-66ce6a1a7cd3" class="">Image segmentation in medical imaging allows for precise delineation of structures like tumors, blood vessels, or organs. This granularity helps in accurate diagnosis, treatment planning, and disease monitoring.</p></li></ol><ol type="1" id="62a36520-3046-4120-84a6-f774c7e0f77c" class="numbered-list" start="3"><li><strong>Agriculture</strong><p id="9a123638-4b05-4b5b-bfd0-fac3cd89fc00" class="">In agriculture, image segmentation can detect and differentiate between crops and weeds. This facilitates targeted herbicide application, ensuring healthier crop growth while minimizing chemical usage.</p></li></ol><p id="78b925aa-88d7-458c-8795-7db5dcc27c3b" class="">
</p><h1 id="10f0b6bb-e53e-401d-a253-e50ab23b2545" class="">Goal</h1><hr id="5aee6960-d24a-4b31-b3f7-d26a8fe8e9fa"/><p id="35fbeec5-c4b6-4feb-8d10-65c21a37cda7" class="">Our goal is to achieve real-time semantic segmentation targets in the CARLA simulation environment using the diverse data collected from it. (e.g., heavy rain, fine dust, sunny, etc.)</p><p id="0087669b-8dd7-43a8-9397-0dd1713d0252" class="">
</p><p id="97906247-1ec1-4586-acca-3ecda43c4524" class=""><strong>The techniques that are utilized in this project:</strong></p><ul id="89a61b93-adb0-40bc-a7b3-d84d1e848117" class="bulleted-list"><li style="list-style-type:disc">Pretrained<ol type="1" id="de02f87d-e25b-406c-a801-c9c9f6207673" class="numbered-list" start="1"><li>DeepLabv3 (backbone: resnet_50)</li></ol><ol type="1" id="7356713f-83a7-4eb6-87d7-93ca4c1eef24" class="numbered-list" start="2"><li>DeepLabv3 (backbone: resnet_50 with Conditional Random Field)</li></ol><ol type="1" id="06beec2d-ec37-44a3-86aa-a2dc9df0e7c8" class="numbered-list" start="3"><li>DeepLabv3 (backbone: mobilenet_v3_large)</li></ol></li></ul><ul id="9cb752c7-0262-48e9-bf05-b2fc44335fd6" class="bulleted-list"><li style="list-style-type:disc">Scratch<ol type="1" id="fe12bbb3-2ed4-448f-9d3b-8d561c13f43d" class="numbered-list" start="1"><li>U-Net</li></ol><ol type="1" id="919093c4-14fb-4882-ba99-ed72691af26e" class="numbered-list" start="2"><li>Mask-RCNN</li></ol></li></ul><p id="b89dafa7-d076-49ad-acf3-6822a37fec72" class="">
</p><h1 id="d53bec9c-d52d-45c7-b227-4d6c9d0eaa2f" class="">Dataset Information</h1><hr id="c5e83cc4-b1bb-4614-8578-21ac8d84df9c"/><p id="21e64f7d-2926-4786-98f6-ebdccc7ebe2a" class="">We performed segmentation using data collected from 10 towns provided by default in CARLA, under three environmental conditions:Â <em><strong>sunny</strong></em>,Â <em><strong>rainy</strong></em>,Â <em><strong>dusty</strong></em>.</p><p id="81abc89d-3654-4225-9105-e63164b20111" class="">The image data has a 1:2 ratio, and we were able to test the generalization performance of segmentation by adjusting to a maximum size of (216 x 512).</p><p id="a99c5764-6f3a-40e1-9d7c-1b10d61f4f31" class="">
</p>
        <h2 id="4695eead-6812-410e-b748-6c75536b3cf4" class="">Class Definition</h2>
        <p id="5245a6c3-744c-4bb4-b349-0ffe209dc0cf" class="">We modified theÂ <strong>28Â </strong><em><strong>classes</strong></em>Â obtained from the official CARLA documentation intoÂ <strong>12Â </strong><em><strong>classes</strong></em>.
        </p>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="fb6020e8-54ae-4b6f-85c8-61fdf82c6c9b" class="code"><code class="language-Python">original_class = {0: &quot;None&quot;, 1 : &quot;Roads&quot;, 2 : &quot;Sidewalks&quot;, 3 : &quot;Buildings&quot;, 4 : &quot;Other&quot;, 5 : &quot;Other&quot;, 6 : &quot;Poles&quot;, 7 : &quot;TrafficLight&quot;, 8 : &quot;TrafficSigns&quot;, 9 : &quot;Vegetation&quot;, 10 : &quot;Roads&quot;, 11 : &quot;None&quot;, 12 : &quot;Pedestrians&quot;, 13 : &quot;Vehicles&quot;, 14 : &quot;Vehicles&quot;, 15 : &quot;Vehicles&quot;, 16 : &quot;Vehicles&quot;, 17 : &quot;Vehicles&quot;, 18 : &quot;Vehicles&quot;, 19 : &quot;Vehicles&quot;, 20 : &quot;Other&quot;, 21 : &quot;Other&quot;, 22 : &quot;Other&quot;, 23 : &quot;Other&quot;, 24 : &quot;RoadLines&quot;, 25 : &quot;Sidewalks&quot;, 26 : &quot;Other&quot;, 27 : &quot;Other&quot;, 28 : &quot;Other&quot;}

remap_class = {0: &quot;None&quot;, 1: &quot;Roads&quot;, 2: &quot;Sidewalks&quot;, 3: &quot;Buildings&quot;, 4: &quot;Other&quot;, 5: &quot;Poles&quot;, 6: &quot;TrafficLight&quot;, 7: &quot;TrafficSigns&quot;, 8: &quot;Vegetation&quot;, 9: &quot;Pedestrians&quot;, 10: &quot;Vehicles&quot;, 11: &quot;RoadLines&quot;}</code></pre><p id="a04e8752-7063-4f89-a4cb-f2472fd0a6fd" class="">You can view the video of the original class and the remapped mask below.</p><p id="4064fc31-e900-4ef7-8b98-318a60ffaf35" class="">
<iframe src="https://wandb.ai/brunoleej/semantic_segmentation/reports/Naive-DeepLabv3_resnet50-Town04-Town06-Town10---Vmlldzo1MjIxNTk0?accessToken=t3jirjy8b4qme2z2j0vie609fl5qxgnkz5zbo07ulchf26jxlbd5u32g2l0dnrzu" width="100%" height="600px" frameborder="0"></iframe>
</p><p id="eca6a617-8708-46fb-a82f-c2fe4e0cbd99" class="">There are several factors in images that influence the training of segmentation.</p><ol type="1" id="fc2463f7-6eae-4fe8-94bf-f72d7cc9e1bb" class="numbered-list" start="1"><li><em><strong>Image Resolution</strong></em>:<ul id="056a0d60-5fc7-4b0f-a896-be17a6cc3037" class="bulleted-list"><li style="list-style-type:disc">High-resolution images provide finer details but increase training time and require more memory.</li></ul><ul id="4e59e88e-bd9f-4fc5-b84e-a77b4dd77ba8" class="bulleted-list"><li style="list-style-type:disc">Reducing resolution simplifies computation but may lead to loss of details.</li></ul></li></ol><ol type="1" id="178a43dd-ad3e-48e6-a17a-bb4e85d207c9" class="numbered-list" start="2"><li><em><strong>Image Quality</strong></em>:<ul id="97a6b8bf-be71-4400-86b1-81ea2833a69b" class="bulleted-list"><li style="list-style-type:disc">Poor quality images (e.g., with lots of noise or low contrast) can impact the accuracy of segmentation.</li></ul></li></ol><ol type="1" id="1e2139c7-3457-4375-9cd6-058a9c4cc3ca" class="numbered-list" start="3"><li><em><strong>Image Augmentation</strong></em>:<ul id="9cfb1159-9040-4e5f-adf9-ab5e3aee8fc8" class="bulleted-list"><li style="list-style-type:disc">Augmentation techniques (like rotation, scaling, flipping, brightness adjustments) help the model generalize better across varied scenarios.</li></ul><ul id="392eea59-b774-4d74-b537-995eb3d9c659" class="bulleted-list"><li style="list-style-type:disc">Over-augmenting can risk overfitting the model.</li></ul></li></ol><ol type="1" id="d850442e-c4c1-44cc-85a0-bbd543d83e76" class="numbered-list" start="4"><li><em><strong>Class Imbalance</strong></em>:<ul id="a0dc974f-e224-4d99-badf-6e51e66fbed2" class="bulleted-list"><li style="list-style-type:disc">If certain classes of pixels vastly outnumber others in an image, it can lead to class imbalance issues. This might degrade segmentation accuracy for some classes.</li></ul></li></ol><ol type="1" id="b24c83f9-5379-41df-800c-17f4a2c70a61" class="numbered-list" start="5"><li><em><strong>Annotation Quality</strong></em>:<ul id="99a1a0b4-1c74-4e64-8ab1-edf4b0237622" class="bulleted-list"><li style="list-style-type:disc">The quality of the ground truth segmentation masks plays a major role in the outcome of the training. Inaccurate masks can decrease training accuracy.</li></ul></li></ol><ol type="1" id="f76ca12c-e710-41f1-91f2-a05e63f494ef" class="numbered-list" start="6"><li><em><strong>Channel Information</strong></em>:<ul id="81d37e1a-852f-42a6-b455-de397a9fe470" class="bulleted-list"><li style="list-style-type:disc">Multi-channel images (e.g., RGB, infra-red, depth, etc.) can provide additional information, potentially improving segmentation accuracy.</li></ul></li></ol><ol type="1" id="98a02f45-b47d-4259-bcf6-3cc9048a373e" class="numbered-list" start="7"><li><em><strong>Variability and Diversity</strong></em>:<ul id="683758e9-a368-4704-b175-97c8ba1583f9" class="bulleted-list"><li style="list-style-type:disc">A diverse set of images in the training set (varying lighting, angles, backgrounds, object sizes, etc.) ensures the model generalizes well in real-world scenarios.</li></ul></li></ol><ol type="1" id="e4b81fba-91b1-4b57-8e83-0b7949d14529" class="numbered-list" start="8"><li><em><strong>Contextual Information</strong></em>:<ul id="a85ed41f-6560-41fc-adfb-c3a218d099c2" class="bulleted-list"><li style="list-style-type:disc">Context in images can assist in predicting the position of specific objects or structures, especially crucial in larger images.</li></ul></li></ol><ol type="1" id="e40ed323-a440-4a86-814f-2d8da58cde9d" class="numbered-list" start="9"><li><em><strong>Spatial Dependencies</strong></em>:<ul id="b1128447-ebda-4145-992d-23d749633111" class="bulleted-list"><li style="list-style-type:disc">Considering the spatial dependencies between pixels within an image can lead to more accurate segmentation outcomes.</li></ul><p id="1ecb6444-4a67-4445-aaa7-82d43783fc15" class="">

    </p>
    </li>
    </ol>
    </div>
</article>
<span class="sans" style="font-size:14px;padding-top:2em"></span>
</body>
</html>